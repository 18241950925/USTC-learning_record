## 部分视图对齐问题的噪声鲁棒对比损失方法
### 论文摘要
本文主要探讨了在现实世界应用中，由于空间、时间或时空异步性等原因，数据只有一部分是相互对齐的问题，即所谓的“部分视图对齐问题”（PVP）。为了解决这个问题，作者提出了同时学习表示和对齐数据的噪声鲁棒对比损失方法。该方法通过构建正样本对和负样本对，并采用随机采样的方式来建立跨视图对应关系。为了消除或减少由随机采样引起的假阴性的影响，作者还提出了一种适应性地防止假阴性主导网络优化的噪声鲁棒对比损失。实验结果表明，该方法在聚类和分类任务中表现出了优异的性能，相比于其他10个多视图方法具有更好的效果。此外，该工作还探讨了与传统噪声标签定义不同的视角对齐错误的定义，这可能会丰富带有噪声标签的学习范式。
### 相关研究
1. 多视图表示学习
通常，大多数现有的MvRL方法高度依赖于多视图数据的完整性和一致性假设。大多数多视图表示学习方法不能处理部分数据缺失问题(PDP)和部分视图对齐问题(PVP)。
与现有研究不同的是，它在类别而不是实例级别执行对齐。
具体地说，在这个研究中定义了两个任意的交叉视图样本来对齐。他们属于同一类别。
1. 对比学习
主要是对于数据构建正负对并且最大化正对之间的相似性，最小化队之间的相似性，与现有研究不同的是，这个研究基于多视图，且带有噪声标签的学习，对于噪声具有鲁棒性。

### 论文方法
1. 方法描述
该论文提出了一种名为Multi-view Contrastive Learning with Noise-robust loss（Mv-CLN）的方法，用于解决多视图数据中的部分对齐问题。该方法首先将部分对齐的数据分为已对齐数据和未对齐数据两部分，并使用对比学习来提高相似正样本之间的距离，同时降低不同负样本之间的距离。然而，在实际应用中，由于噪声的存在，对比学习可能会受到干扰，导致性能下降。因此，该方法引入了噪声鲁棒的对比学习损失函数，以减少或消除噪声的影响。

2. 方法改进
与传统的对比学习方法相比，Mv-CLN通过随机采样构造负样本的方式避免了人为构建负样本时可能出现的错误匹配问题。此外，该方法还采用了噪声鲁棒的对比学习损失函数，以减少或消除噪声的影响，从而提高了模型的性能。

3. 解决的问题
该方法主要解决了多视图数据中的部分对齐问题，并通过对比学习和噪声鲁棒的对比学习损失函数来提高模型的性能。在实验中，该方法在多个数据集上都取得了比传统对比学习方法更好的结果，证明了其有效性和实用性。
![alt text](image.png)

### 论文实验
本文主要介绍了多视图聚类网络（MvCLN）的实验结果和分析。实验使用了四个常用的多视图数据集，并通过聚类和分类任务来评估学习到的表示方法。在实验中，与十种最先进的多视图聚类方法进行了比较，包括cca、kcca、dccae等。实验结果表明，在第一种设置下，即部分视图对齐的数据上，MvCLN显著优于所有测试方法，尤其是在Caltech-101和Reuters数据集上的表现更好。在第二种设置下，即完全视图对齐的数据上，MvCLN仍然能够取得竞争力的结果。此外，文章还进行了参数分析和AUC评估等实验，证明了MvCLN的有效性和鲁棒性。
![alt text](image-1.png)

### 论文总结
1. 文章优点
该论文提出了一种新的解决多视图数据中部分视图缺失问题的方法，即通过对比学习实现类别级别的对齐，而不是实例级别对齐。这种方法比传统的基于匈牙利算法的对齐方法更加高效且可扩展，并且能够处理噪声标签的问题。此外，该论文还提出了一种新颖的噪声鲁棒对比损失函数，可以有效地缓解或消除随机采样引入的假负样本的影响。


1. 方法创新点
该论文的主要贡献在于提出了一个新方法来解决多视图数据中的部分视图缺失问题。与传统的基于匈牙利算法的对齐方法不同，该方法实现了类别级别的对齐，这使得该方法在大规模数据集上具有更好的可扩展性和效率。此外，该论文还提出了一种新颖的噪声鲁棒对比损失函数，可以在一定程度上减轻或消除随机采样引入的假负样本的影响。

1. 未来展望
该论文提出的多视图对比学习框架和噪声鲁棒对比损失函数为解决多视图数据中的部分视图缺失问题提供了一个全新的思路。未来的研究可以从以下几个方面展开：首先，可以进一步探索如何利用其他类型的数据（如文本、音频等）来提高模型的性能；其次，可以研究如何将该方法应用于更广泛的应用领域，例如图像检索、目标跟踪等；最后，可以考虑如何结合其他技术（如深度强化学习）来进一步提高模型的性能。

### 课程关联
1. 损失函数
这个论文设计的损失函数为 Noise-robust Contrastive Loss![alt text](image-2.png)
这与课程中提到的Noise Contrastive Estimation有所不同
2. 数据对齐
课程内也提到过数据对齐的问题，不过实验的数据来源比较单一并不涉及这一点。
这个研究主要是多视图的数据对齐，将多个来源的数据进行对齐
3. 对比学习
课程在自监督学习那一节讲到了这一点，其中就有基于对比学习的自监督学习

    • 利用数据之间的对比关系进行表示学习

    • 让像的样本所得表示差异小，让不像的样本所得表示差异大

    • 期望学到更通用的知识，与辅助任务无关

### 阅读收获
对于对比学习这一方法有了更深入的了解，对于多视图的对齐问题的前沿的解决方法醉了一定调研，明白了数据对齐的重要性。在文章的method部分了解了损失函数的具体构建，以及为什么要这么构建。在实验部分学习到了对于论文研究结果的多中评价指标，其中也包括课程实验用到的ACC等。最后了解了数据对齐中的噪声这一问题，这是在课程内容中没有提到的。
